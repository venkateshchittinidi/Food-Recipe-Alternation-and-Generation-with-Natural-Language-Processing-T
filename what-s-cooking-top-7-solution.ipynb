{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presenting a solution to get into top 7% of leaderboard using Support Vector Classifier with an accuracy score of 0.81063"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://media3.s-nbcnews.com/j/newscms/2019_41/3044956/191009-cooking-vegetables-al-1422_ae181a762406ae9dce02dd0d5453d1ba.nbcnews-fp-1200-630.jpg\" alt=\"Cooking Image from Google\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import zipfile\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['train','test']:\n",
    "    with zipfile.ZipFile(\"../input/whats-cooking/{}.json.zip\".format(t),\"r\") as z:\n",
    "        z.extractall(\".\")\n",
    "    \n",
    "with open('./train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    \n",
    "with open('./test.json') as test_file:\n",
    "    test = json.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "test_df = pd.DataFrame(test)\n",
    "\n",
    "test_ids = test_df['id']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.isnull().sum() / len(df))*100 # No null values in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_df.isnull().sum() / len(test_df))*100 # No null values in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "per_vals = round(df[\"cuisine\"].value_counts(normalize=True)*100, 2)\n",
    "for i, v in enumerate(per_vals):\n",
    "    ax.text(v + 3, i + .25, str(v)+\"%\", color='blue', fontweight='bold')\n",
    "df[\"cuisine\"].value_counts().plot.barh(ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(22,7))\n",
    "extensive_ing_list = []\n",
    "for x in df['ingredients']:\n",
    "    for y in x:\n",
    "        extensive_ing_list.append(y)\n",
    "        \n",
    "extensive_ing_list = pd.Series(extensive_ing_list)\n",
    "extensive_ing_list.value_counts().sort_values(ascending=False).head(30).plot.bar(ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating ingredients per cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine = df[\"cuisine\"].unique()\n",
    "\n",
    "all_cus = dict()\n",
    "for cs in cuisine:\n",
    "    i = []\n",
    "    for ing_list in df[df['cuisine']==cs]['ingredients']:\n",
    "        for ing in ing_list:\n",
    "            i.append(ing)\n",
    "    all_cus[cs] = i\n",
    "\n",
    "all_cus.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 MOST USED INGREDIENTS- CUISINE WISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in all_cus.keys():\n",
    "    fig, ax = plt.subplots(figsize=(25,2))\n",
    "    pd.Series(all_cus[key]).value_counts().head(25).plot.bar(ax=ax, title=key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 LEAST USED INGREDIENTS- CUISINE WISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in all_cus.keys():\n",
    "#     fig, ax = plt.subplots(figsize=(25,2))\n",
    "#     pd.Series(all_cus[key]).value_counts().tail(25).plot.bar(ax=ax, title=key)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    \n",
    "    def process_string(x):\n",
    "        x = [\" \".join([WordNetLemmatizer().lemmatize(q) for q in p.split()]) for p in x] #Lemmatization\n",
    "        x = list(map(lambda x: re.sub(r'\\(.*oz.\\)|crushed|crumbles|ground|minced|powder|chopped|sliced','', x), x))\n",
    "        x = list(map(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x), x))   # To remove everything except a-z and A-Z\n",
    "        x = \" \".join(x)                                 # To make list element a string element \n",
    "        x = x.lower()\n",
    "        return x\n",
    "    \n",
    "    df = df.drop('id',axis=1)\n",
    "    df['ingredients'] = df['ingredients'].apply(process_string)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuisine_cumulated_ingredients(df):\n",
    "    cuisine_df = pd.DataFrame(columns=['ingredients'])\n",
    "\n",
    "    for cus in cuisine:\n",
    "        st = \"\"\n",
    "        for x in df[df.cuisine == cus]['ingredients']:\n",
    "            st += x\n",
    "            st += \" \"\n",
    "        cuisine_df.loc[cus,'ingredients'] = st\n",
    "\n",
    "    cuisine_df = cuisine_df.reset_index()\n",
    "    cuisine_df = cuisine_df.rename(columns ={'index':'cuisine'})\n",
    "    return cuisine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_df(df)\n",
    "test_df = preprocess_df(test_df)\n",
    "\n",
    "cuisine_df = get_cuisine_cumulated_ingredients(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df['ingredients']\n",
    "target = df['cuisine']\n",
    "test = test_df['ingredients']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(train, test=None):\n",
    "    cv = CountVectorizer()\n",
    "    train = cv.fit_transform(train)\n",
    "    if test is not None:\n",
    "        test = cv.transform(test)\n",
    "        return train, test, cv\n",
    "    else:\n",
    "        return train, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cv , test_cv, cv = count_vectorizer(train,test)\n",
    "# cuisine_data_cv, cuisine_cv = count_vectorizer(cuisine_df['ingredients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFiDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(train, test=None):\n",
    "    tfidf = TfidfVectorizer(stop_words='english',\n",
    "                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n",
    "                             max_df = .57 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)\n",
    "    train = tfidf.fit_transform(train)\n",
    "    if test is not None:\n",
    "        test = tfidf.transform(test)\n",
    "        return train, test, tfidf\n",
    "    else:\n",
    "        return train, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf, test_tfidf, tfidf = tfidf_vectorizer(train,test)\n",
    "cuisine_data_tfidf, cuisine_tfidf = tfidf_vectorizer(cuisine_df['ingredients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster as a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 20 different types of cuisine to classify. It gives an intuition that certain groups of cuisine may have much more similarity than others. We can try to find such groups as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import KernelPCA,PCA,TruncatedSVD\n",
    "\n",
    "def get_kmeans_wcss(data, n_limit=15):\n",
    "    wcss = [] #Within cluster sum of squares (WCSS)\n",
    "    for i in range(1,n_limit):\n",
    "        km = KMeans(init='k-means++', n_clusters=i, n_init=10)\n",
    "        km.fit(data)\n",
    "        wcss.append(km.inertia_)\n",
    "    plt.title(\"Elbow Method\")\n",
    "    plt.plot(range(1, n_limit), wcss)\n",
    "    plt.xlabel(\"Number of clusters\")\n",
    "    plt.ylabel(\"WCSS\")\n",
    "    return wcss\n",
    "    \n",
    "    \n",
    "def kmeans(data, n):\n",
    "    km = KMeans(init='k-means++', n_clusters=n, n_init=10)\n",
    "    km = km.fit(data)\n",
    "    return km.predict(data), km \n",
    "\n",
    "\n",
    "def get_PCA(data, n_components=2):\n",
    "    pca = PCA(n_components = n_components)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(explained_variance)\n",
    "    return reduced_data, pca, explained_variance\n",
    "\n",
    "def get_kernel_PCA(data, n_components=2, kernel='rbf'):\n",
    "    kpca = KernelPCA(n_components = 2, kernel = kernel)\n",
    "    reduced_data = kpca.fit_transform(data)\n",
    "    explained_variance = kpca.explained_variance_ratio_\n",
    "    print(explained_variance)\n",
    "    return reduced_data, kpca, explained_variance\n",
    "\n",
    "def get_TSVD(data, n_components=2, n_ittr=5, algorithm='randomized'):\n",
    "    tsvd = TruncatedSVD(n_components=n_components, n_iter=n_ittr, algorithm=algorithm)\n",
    "    reduced_data = tsvd.fit_transform(data)\n",
    "    explained_variance = tsvd.explained_variance_ratio_\n",
    "    print(explained_variance)\n",
    "    return reduced_data, tsvd, explained_variance\n",
    "\n",
    "\n",
    "\n",
    "def create_pca_graph(cluster_pca, red_pca, n_clus):\n",
    "\n",
    "    c_mask = []\n",
    "    c_x = []\n",
    "    c_y = []\n",
    "    \n",
    "    for i in range(0,n_clus):\n",
    "        c_mask.append([x for x in cluster_pca==i])\n",
    "    \n",
    "    for i in range(0,n_clus):\n",
    "        c_x.append([a[0] for a, b in zip(red_pca, c_mask[i]) if b])\n",
    "        c_y.append([a[1] for a, b in zip(red_pca, c_mask[i]) if b])\n",
    "\n",
    "    colours = ['red','blue','green','orange','purple','cyan','black','magenta']\n",
    "    \n",
    "    for i in range(0,n_clus):\n",
    "        plt.scatter(c_x[i], c_y[i], s=30, c=colours[i], label='Cluster {}'.format(i))\n",
    "        \n",
    "        \n",
    "#     for i in range(0,20):\n",
    "#         label = label_list[i]\n",
    "#         plt.annotate(label, (c_x[i],c_y[i]), textcoords=\"offset points\", xytext=(0,10), # distance from text to points (x,y)\n",
    "#                      ha='center') # horizontal alignment can be left, right or center\n",
    "        \n",
    "     \n",
    "    plt.title(\"Clusters of PCA\")\n",
    "    plt.xlabel(\"PCA 1\")\n",
    "    plt.ylabel(\"PCA 2\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize clusters, let us reduce the data using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red_tsvd, tsvd, var_tsvd = get_TSVD(train_cv,2)  #Used because train_cv is a sparse matrix. PCA won't work\n",
    "# red_pca, pca, var_pca = get_PCA((train_cv).toarray(),2)\n",
    "# red_pca, pca, var_pca = get_PCA((train_tfidf).toarray(),2)\n",
    "# red_tsvd, tsvd, var_tsvd = get_TSVD(train_tfidf,2)  #Used because train_tfidf is a sparse matrix. PCA won't work\n",
    "# red_kpca, kpca, var_kpca = get_kernel_PCA(train_cv,2)  #Uses excessive RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_cuisine_pca, cus_pca, var_cus_pca = get_PCA((cuisine_data_tfidf).toarray(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wcss_pca = get_kmeans_wcss(red_cuisine_pca,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WCSS for reduced cuisine dataset shows that number of clusters = 3 should be an apt choice (elbow point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cus_pca, km_cus_pca = kmeans(red_cuisine_pca,3)\n",
    "cluster_cus_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pca_graph(cluster_cus_pca, red_cuisine_pca, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice there are 3 clusters of cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuisine_df[cluster_cus_pca==0]['cuisine']\n",
    "# cuisine_df[cluster_cus_pca==1]['cuisine']\n",
    "# cuisine_df[cluster_cus_pca==2]['cuisine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLUSTER 1: <br>**\n",
    "> GREEK<br>\n",
    "> SPANISH<br>\n",
    "> ITALIAN<br>\n",
    "> FRENCH<br>\n",
    "> MOROCCAN<br>\n",
    "> RUSSIAN<br>\n",
    "\n",
    "<br><br>\n",
    "**CLUSTER 2: <br>**\n",
    "> FILIPINO<br>\n",
    "> CHINESE<br>\n",
    "> THAI<br>\n",
    "> VIETNAMESE<br>\n",
    "> KOREAN<br>\n",
    "\n",
    "<br><br>\n",
    "**CLUSTER 3: <br>**\n",
    "> SOUTHERN US<br>\n",
    "> INDIAN<br>\n",
    "> JAMAICAN<br>\n",
    "> MEXICAN<br>\n",
    "> BRITISH<br>\n",
    "> CAJUN CREOLE<br>\n",
    "> BRAZILIAN<br>\n",
    "> JAPANESE<br>\n",
    "> IRISH<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Actual Clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Don't add cluster for best results. (Skip this section. Move to Model Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wcss = get_kmeans_wcss(train_tfidf,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WCSS shows number of clusters = 19 can be an apt choice (elbow point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster, km = kmeans(train_tfidf,19) # train_cv or train_tfidf\n",
    "cluster_test = km.predict(test_tfidf)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(cluster.reshape(-1, 1))\n",
    "cluster_encoded = enc.transform(cluster.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_test_encoded = enc.transform(cluster_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding cluster as a feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_nonsparse = np.append((train_tfidf).toarray(), cluster_encoded, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf_nonsparse = np.append((test_tfidf).toarray(), cluster_test_encoded, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING DATASET: Added cluster of shape {} to train_cv of shape {} as a column\".format(cluster_encoded.shape, train_tfidf.shape))\n",
    "print(\"TESTING DATASET: Added cluster of shape {} to test_cv of shape {} as a column\".format(cluster_test_encoded.shape, test_tfidf.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# train = sparse.csr_matrix(train_tfidf_nonsparse)\n",
    "# test = sparse.csr_matrix(test_tfidf_nonsparse)\n",
    "\n",
    "train = train_tfidf # USE THIS FOR BEST RESULTS (0.8106)\n",
    "test = test_tfidf # USE THIS FOR BEST RESULTS (0.8106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "param_grid = {'C': [0.001, 0.1, 1, 10, 50, 100, 500, 1000, 5000],  \n",
    "              'penalty': ['l1','l2'],\n",
    "             'loss': ['hinge','squared hinge']} \n",
    "\n",
    "grid = GridSearchCV(LinearSVC(), param_grid, refit = True, verbose = 3, n_jobs=-1, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid.fit(train, target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "def evalfn(C, gamma):\n",
    "    s = SVC(C=float(C), gamma=float(gamma), kernel='rbf', class_weight='balanced')\n",
    "    f = cross_val_score(s, train, target, cv=5, scoring='f1_micro')\n",
    "    return f.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "new_opt = BayesianOptimization(evalfn, {'C': (0.1, 1000),  \n",
    "              'gamma': (0.0001, 1)  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### OPTIMIZED PARAMETERS ARE SHOWN BELOW ###\n",
    "##  HYPER PARAMETER OPT IS TIME CONSUMING ##\n",
    "############################################\n",
    "\n",
    "# %%time\n",
    "# new_opt.maximize(n_iter=15, init_points=3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_opt.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED PARAMETERS\n",
    "# {'target': 0.7945391461758937,\n",
    "#  'params': {'C': 604.5300203551828, 'gamma': 0.9656489284085462}}\n",
    "\n",
    "# With cluster(n=19) as a parameter:\n",
    "# {'target': 0.7940917661847894,\n",
    "#  'params': {'C': 509.674609734803, 'gamma': 0.724238238886398}}\n",
    "\n",
    "C = 604.5300203551828\n",
    "gamma = 0.9656489284085462\n",
    "\n",
    "clf = SVC(C=float(C), gamma=float(gamma), kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"MODEL SAVED AT {}\".format(now))\n",
    "model_name = \"SVC-whats-cooking-trial-final2-{}.pickle.dat\".format(now)\n",
    "pickle.dump(clf, open(model_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = pickle.load(open(\"SVC-whats-cooking-trial-final2-{}.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
